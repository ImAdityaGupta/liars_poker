{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup & Imports\n",
        "Establish the repo context, import core helpers, and fix a RNG for reproducible snippets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: C:\\Users\\adidh\\Documents\\liars_poker\n",
            "Artifacts root: C:\\Users\\adidh\\Documents\\liars_poker\\artifacts\n"
          ]
        }
      ],
      "source": [
        "import os, sys, random\n",
        "from pathlib import Path\n",
        "\n",
        "def find_repo_root(start_dir: str) -> str:\n",
        "    cur = Path(start_dir).resolve()\n",
        "    for _ in range(6):\n",
        "        if (cur / \"liars_poker\").is_dir() or (cur / \"pyproject.toml\").exists():\n",
        "            return str(cur)\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return str(Path(start_dir).resolve())\n",
        "\n",
        "NB_DIR = Path.cwd()\n",
        "REPO_ROOT = Path(find_repo_root(NB_DIR))\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "ARTIFACTS_ROOT = REPO_ROOT / \"artifacts\"\n",
        "ARTIFACTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "from liars_poker import GameSpec, Env, Rules, InfoSet, CALL, RandomPolicy, TabularPolicy, CommitOnceMixture\n",
        "from liars_poker.core import generate_deck, card_display, possible_starting_hands, env_hash\n",
        "from liars_poker.eval.match import play_match, eval_both_seats, exact_eval_tabular_both_seats\n",
        "from liars_poker.algo.br_mc import best_response_mc\n",
        "from liars_poker.algo.br_exact import best_response_exact\n",
        "from liars_poker.policies.base import Policy\n",
        "\n",
        "pp = pprint = __import__(\"pprint\").pprint\n",
        "rng = random.Random(42)\n",
        "print(\"Repo root:\", REPO_ROOT)\n",
        "print(\"Artifacts root:\", ARTIFACTS_ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Game Spec & Deck Helpers\n",
        "Show how a game is parameterized and how card encodings/decks look under different specs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plain: json={\"claim_kinds\": [\"RankHigh\"], \"hand_size\": 1, \"ranks\": 6, \"suit_symmetry\": false, \"suits\": 1} hash=d47c79a8bb2b\n",
            "pair: json={\"claim_kinds\": [\"RankHigh\", \"Pair\"], \"hand_size\": 1, \"ranks\": 6, \"suit_symmetry\": true, \"suits\": 2} hash=9f85e7d094bf\n"
          ]
        }
      ],
      "source": [
        "spec_plain = GameSpec(ranks=6, suits=1, hand_size=1, claim_kinds=(\"RankHigh\",))\n",
        "spec_pair = GameSpec(ranks=6, suits=2, hand_size=1, claim_kinds=(\"RankHigh\", \"Pair\"), suit_symmetry=True)\n",
        "for label, spec in [(\"plain\", spec_plain), (\"pair\", spec_pair)]:\n",
        "    print(f\"{label}: json={spec.to_json()} hash={env_hash(spec)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deck: (1, 1, 2, 2, 3, 3)\n",
            "Display deck: ['1', '1', '2', '2', '3', '3']\n",
            "Starting hands (sorted): [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n"
          ]
        }
      ],
      "source": [
        "tiny_spec = GameSpec(ranks=3, suits=2, hand_size=2, claim_kinds=(\"RankHigh\", \"Pair\"), suit_symmetry=True)\n",
        "deck = generate_deck(tiny_spec)\n",
        "print(\"Deck:\", deck)\n",
        "print(\"Display deck:\", [card_display(c, tiny_spec) for c in deck])\n",
        "hands = possible_starting_hands(tiny_spec)\n",
        "print(\"Starting hands (sorted):\", hands)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rules & Legal Actions\n",
        "Inspect claim ordering and how actions are parsed/rendered for a sample history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claims (ordered): ['RankHigh:1', 'RankHigh:2', 'RankHigh:3', 'Pair:1', 'Pair:2', 'Pair:3']\n",
            "Round-trip parse/render: idx 1 -> RankHigh:2\n"
          ]
        }
      ],
      "source": [
        "rules = Rules(tiny_spec)\n",
        "claims = [rules.render_action(i) for i in range(len(rules.claims))]\n",
        "print(\"Claims (ordered):\", claims)\n",
        "roundtrip = rules.parse_action(\"RankHigh:2\")\n",
        "print(\"Round-trip parse/render: idx\", roundtrip, \"->\", rules.render_action(roundtrip))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "History ends with RankHigh:2; legal ids: (-1, 2, 3, 4, 5)\n",
            "Rendered: ['CALL', 'RankHigh:3', 'Pair:1', 'Pair:2', 'Pair:3']\n"
          ]
        }
      ],
      "source": [
        "last_claim_idx = rules.parse_action(\"RankHigh:2\")\n",
        "legal_ids = rules.legal_actions_from_last(last_claim_idx)\n",
        "print(\"History ends with RankHigh:2; legal ids:\", legal_ids)\n",
        "print(\"Rendered:\", [rules.render_action(a) if a != CALL else \"CALL\" for a in legal_ids])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Step-Through\n",
        "Walk a tiny game with fixed hands to show observations, legal moves, and terminal resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hand': (1,),\n",
            " 'history': (),\n",
            " 'legal_actions': [0, 1, 2, 3, 4, 5],\n",
            " 'terminal': False,\n",
            " 'to_play': 'P1',\n",
            " 'winner': None}\n"
          ]
        }
      ],
      "source": [
        "step_spec = tiny_spec\n",
        "step_rules = Rules(step_spec)\n",
        "env = Env(step_spec, seed=0)\n",
        "obs = env.reset(hands=((1,), (2,)))  # P1 holds rank 1, P2 holds rank 2\n",
        "pp({k: obs[k] for k in (\"to_play\", \"hand\", \"legal_actions\", \"history\", \"terminal\", \"winner\")})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P1 claims RankHigh:2\n",
            "{'hand': (2,),\n",
            " 'history': (1,),\n",
            " 'legal_actions': [-1, 2, 3, 4, 5],\n",
            " 'terminal': False,\n",
            " 'to_play': 'P2',\n",
            " 'winner': None}\n"
          ]
        }
      ],
      "source": [
        "action_claim = step_rules.parse_action(\"RankHigh:2\")\n",
        "obs = env.step(action_claim)\n",
        "print(\"P1 claims RankHigh:2\")\n",
        "pp({k: obs[k] for k in (\"to_play\", \"hand\", \"legal_actions\", \"history\", \"terminal\", \"winner\")})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P2 calls; resolving...\n",
            "{'hand': (2,),\n",
            " 'history': (1, -1),\n",
            " 'terminal': True,\n",
            " 'to_play': 'P2',\n",
            " 'winner': 'P1'}\n"
          ]
        }
      ],
      "source": [
        "obs = env.step(CALL)\n",
        "print(\"P2 calls; resolving...\")\n",
        "pp({k: obs[k] for k in (\"to_play\", \"hand\", \"history\", \"terminal\", \"winner\")})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policies Overview\n",
        "Bind/query simple policies and compare a custom tabular distribution to the random default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomPolicy distribution: {-1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.2}\n"
          ]
        }
      ],
      "source": [
        "rand_pol = RandomPolicy(); rand_pol.bind_rules(step_rules)\n",
        "opening_iset = env.infoset_key(\"P1\")\n",
        "print(\"RandomPolicy distribution:\", rand_pol.action_probs(opening_iset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TabularPolicy distribution: {-1: 0.7, 2: 0.3, 3: 0.0, 4: 0.0, 5: 0.0}\n"
          ]
        }
      ],
      "source": [
        "tab_pol = TabularPolicy(); tab_pol.bind_rules(step_rules)\n",
        "legal = step_rules.legal_actions_for(opening_iset)\n",
        "custom_dist = {legal[0]: 0.7}\n",
        "if len(legal) > 1:\n",
        "    custom_dist[legal[1]] = 0.3\n",
        "\n",
        "tab_pol.set(opening_iset, custom_dist)\n",
        "print(\"TabularPolicy distribution:\", tab_pol.action_probs(opening_iset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Response (Monte Carlo vs Exact)\n",
        "Compare MC and exact best responses on a tiny game to see how openings differ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MC BR opening distribution: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0}\n"
          ]
        }
      ],
      "source": [
        "spec_br = GameSpec(ranks=3, suits=2, hand_size=1, claim_kinds=(\"RankHigh\", \"Pair\"), suit_symmetry=True)\n",
        "rules_br = Rules(spec_br)\n",
        "base = RandomPolicy(); base.bind_rules(rules_br)\n",
        "\n",
        "policy_mc = best_response_mc(spec_br, base, episodes=200, epsilon=0.1, min_visits_per_action=1, annotate=\"memory\", seed=7)\n",
        "opening_iset_br = InfoSet(pid=0, hand=possible_starting_hands(spec_br)[0], history=())\n",
        "print(\"MC BR opening distribution:\", policy_mc.action_probs(opening_iset_br))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact BR opening distribution: {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0}\n"
          ]
        }
      ],
      "source": [
        "policy_exact, br_comp = best_response_exact(spec_br, base)\n",
        "policy_exact.bind_rules(rules_br)\n",
        "print(\"Exact BR opening distribution:\", policy_exact.action_probs(opening_iset_br))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MC best action: RankHigh:1\n",
            "Exact best action: RankHigh:1\n"
          ]
        }
      ],
      "source": [
        "mc_best = max(policy_mc.action_probs(opening_iset_br).items(), key=lambda x: x[1])[0]\n",
        "ex_best = max(policy_exact.action_probs(opening_iset_br).items(), key=lambda x: x[1])[0]\n",
        "print(\"MC best action:\", rules_br.render_action(mc_best))\n",
        "print(\"Exact best action:\", rules_br.render_action(ex_best))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Algorithms Showcase\n",
        "Lightweight demos of the iterative averaging loop and a single Double Oracle population growth step (from the post_oct13 notebooks).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting average policy: RandomPolicy\n"
          ]
        }
      ],
      "source": [
        "avg_spec = GameSpec(ranks=3, suits=2, hand_size=1, claim_kinds=(\"RankHigh\", \"Pair\"), suit_symmetry=True)\n",
        "avg_rules = Rules(avg_spec)\n",
        "avg_policy = RandomPolicy(); avg_policy.bind_rules(avg_rules)\n",
        "eta = 0.2  # small step size for demo\n",
        "print(\"Starting average policy: RandomPolicy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mixed policy weights (avg, br): [0.8, 0.2]\n",
            "Mixed distribution at opening: {0: 0.33333333333333337, 1: 0.13333333333333333, 2: 0.13333333333333333, 3: 0.13333333333333333, 4: 0.13333333333333333, 5: 0.13333333333333333}\n"
          ]
        }
      ],
      "source": [
        "br_once, _ = best_response_exact(avg_spec, avg_policy)\n",
        "br_once.bind_rules(avg_rules)\n",
        "new_avg = CommitOnceMixture([avg_policy, br_once], [1 - eta, eta]); new_avg.bind_rules(avg_rules)\n",
        "print(\"Mixed policy weights (avg, br):\", new_avg.weights)\n",
        "probe_iset = InfoSet(pid=0, hand=possible_starting_hands(avg_spec)[0], history=())\n",
        "print(\"Mixed distribution at opening:\", new_avg.prob_dist_at_infoset(probe_iset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial payoff matrix (1x1): [[0.51]]\n"
          ]
        }
      ],
      "source": [
        "do_spec = avg_spec\n",
        "do_rules = avg_rules\n",
        "seed_policy = RandomPolicy(); seed_policy.bind_rules(do_rules)\n",
        "pop_row = [seed_policy]\n",
        "pop_col = [seed_policy]\n",
        "val_init = eval_both_seats(do_spec, pop_row[0], pop_col[0], episodes=200, seed=11)[\"P1\"]\n",
        "print(\"Initial payoff matrix (1x1):\", [[round(val_init, 3)]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expanded payoff matrix (row player value):\n",
            "[[0.46, 0.105], [0.855, 0.43]]\n",
            "Population sizes -> row: 2 col: 2\n"
          ]
        }
      ],
      "source": [
        "br_row, _ = best_response_exact(do_spec, pop_col[-1]); br_row.bind_rules(do_rules)\n",
        "br_col, _ = best_response_exact(do_spec, pop_row[-1]); br_col.bind_rules(do_rules)\n",
        "pop_row.append(br_row)\n",
        "pop_col.append(br_col)\n",
        "\n",
        "def payoff(a, b, seed):\n",
        "    return eval_both_seats(do_spec, a, b, episodes=200, seed=seed)[\"P1\"]\n",
        "\n",
        "P = [\n",
        "    [round(payoff(pop_row[0], pop_col[0], 21), 3), round(payoff(pop_row[0], pop_col[1], 22), 3)],\n",
        "    [round(payoff(pop_row[1], pop_col[0], 23), 3), round(payoff(pop_row[1], pop_col[1], 24), 3)],\n",
        "]\n",
        "print(\"Expanded payoff matrix (row player value):\")\n",
        "pp(P)\n",
        "print(\"Population sizes -> row:\", len(pop_row), \"col:\", len(pop_col))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy Persistence\n",
        "Save/load a policy using the efficient binary helpers and confirm behavior after reload.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_dir = ARTIFACTS_ROOT / \"overview_demo\" / \"random_policy\"\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "rand_pol.store_efficiently(str(save_dir))\n",
        "print(\"Saved files:\", sorted(os.listdir(save_dir)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded, loaded_spec = Policy.load_policy(str(save_dir))\n",
        "opening_again = InfoSet(pid=0, hand=(1,), history=())\n",
        "print(\"Loaded spec equals original:\", loaded_spec == step_spec)\n",
        "print(\"Loaded policy distribution (opening):\", loaded.action_probs(opening_again))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Helpers\n",
        "Quickly evaluate matchups via sampling and exact tabular evaluation on tiny specs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled wins (random vs BR): {'P1': 112, 'P2': 88}\n"
          ]
        }
      ],
      "source": [
        "a_sample = play_match(Env(step_spec), rand_pol, br_once, episodes=200, seed=5)\n",
        "print(\"Sampled wins (random vs BR):\", a_sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact evaluation (uniform tabular vs exact BR): {'P1': 0.5, 'P2': 0.5}\n"
          ]
        }
      ],
      "source": [
        "tab_rand = TabularPolicy(); tab_rand.bind_rules(step_rules)\n",
        "res_exact = exact_eval_tabular_both_seats(step_spec, tab_rand, policy_exact)\n",
        "print(\"Exact evaluation (uniform tabular vs exact BR):\", res_exact)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact evaluation (mixed vs exact BR on avg_spec): {'P1': 0.1927407407407407, 'P2': 0.8072592592592593}\n"
          ]
        }
      ],
      "source": [
        "mixed_tab = new_avg.to_tabular()\n",
        "mixed_tab.bind_rules(avg_rules)\n",
        "res_mix = exact_eval_tabular_both_seats(avg_spec, mixed_tab, policy_exact)\n",
        "print(\"Exact evaluation (mixed vs exact BR on avg_spec):\", res_mix)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
