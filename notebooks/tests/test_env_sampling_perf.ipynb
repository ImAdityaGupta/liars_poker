{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c52d33c",
      "metadata": {},
      "source": [
        "# Sampling & Env Performance Checks\n",
        "Guardrails before refactoring sampling / Env internals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "132fe1ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: C:\\Users\\adidh\\Documents\\liars_poker\n"
          ]
        }
      ],
      "source": [
        "import os, sys, time, statistics, random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "def find_repo_root(start_dir: str) -> str:\n",
        "    cur = Path(start_dir).resolve()\n",
        "    for _ in range(6):\n",
        "        if (cur / 'liars_poker').is_dir() or (cur / 'pyproject.toml').exists():\n",
        "            return str(cur)\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return str(Path(start_dir).resolve())\n",
        "\n",
        "NB_DIR = Path.cwd()\n",
        "REPO_ROOT = Path(find_repo_root(NB_DIR))\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "print('Repo root:', REPO_ROOT)\n",
        "\n",
        "from liars_poker import GameSpec, Env, Rules\n",
        "from liars_poker.infoset import InfoSet, CALL\n",
        "from liars_poker.policies import RandomPolicy, TabularPolicy\n",
        "from liars_poker.policies.tabular_dense import DenseTabularPolicy\n",
        "from liars_poker.eval.match import play_match, eval_seats_split\n",
        "from liars_poker.algo.br_exact import best_response_exact\n",
        "\n",
        "rng = random.Random(123)\n",
        "\n",
        "def freq_from_samples(samples):\n",
        "    c = Counter(samples)\n",
        "    total = sum(c.values())\n",
        "    return {k: v / total for k, v in c.items()}\n",
        "\n",
        "def assert_close_dist(got, expected, tol=0.08):\n",
        "    for action, exp in expected.items():\n",
        "        g = got.get(action, 0.0)\n",
        "        assert abs(g - exp) <= tol, f\"{action}: {g:.3f} vs {exp:.3f}\"\n",
        "\n",
        "def bench(fn, runs=5, warmup=1):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    times = []\n",
        "    for _ in range(runs):\n",
        "        t0 = time.perf_counter()\n",
        "        fn()\n",
        "        times.append(time.perf_counter() - t0)\n",
        "    return times\n",
        "\n",
        "def summarize(times):\n",
        "    if not times:\n",
        "        return None\n",
        "    return {\n",
        "        'mean': statistics.mean(times),\n",
        "        'stdev': statistics.stdev(times) if len(times) > 1 else 0.0,\n",
        "        'median': statistics.median(times),\n",
        "        'min': min(times),\n",
        "        'max': max(times),\n",
        "    }\n",
        "\n",
        "def report(label, times, units='s', per=None):\n",
        "    stats = summarize(times)\n",
        "    if stats is None:\n",
        "        return\n",
        "    mean = stats['mean']\n",
        "    extra = ''\n",
        "    if per is not None and per > 0:\n",
        "        extra = f\", per={mean / per:.6f} {units}\"\n",
        "    print(f\"{label}: mean={mean:.6f}{units} std={stats['stdev']:.6f}{units} median={stats['median']:.6f}{units}{extra}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1068c60c",
      "metadata": {},
      "source": [
        "## Correctness: Sampling determinism\n",
        "Ensure sampling respects RNG and legal actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6b49bf06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Legal actions: (0, 1, 2)\n",
            "Samples1 == Samples2: True [0, 2, 2, 0, 1, 1, 1, 2]\n",
            "Deterministic sampling OK.\n"
          ]
        }
      ],
      "source": [
        "spec = GameSpec(ranks=3, suits=1, hand_size=1, claim_kinds=('RankHigh',))\n",
        "rules = Rules(spec)\n",
        "pol = RandomPolicy(); pol.bind_rules(rules)\n",
        "iset = InfoSet(pid=0, hand=(1,), history=())\n",
        "legal = rules.legal_actions_for(iset)\n",
        "print('Legal actions:', legal)\n",
        "\n",
        "rng1 = random.Random(1)\n",
        "samples1 = [pol.sample(iset, rng1) for _ in range(8)]\n",
        "rng2 = random.Random(1)\n",
        "samples2 = [pol.sample(iset, rng2) for _ in range(8)]\n",
        "print('Samples1 == Samples2:', samples1 == samples2, samples1)\n",
        "assert all(a in legal for a in samples1)\n",
        "assert samples1 == samples2\n",
        "print('Deterministic sampling OK.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3651e5b",
      "metadata": {},
      "source": [
        "## Correctness: Sampling distribution vs action_probs\n",
        "Empirically check that sampling matches intended probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c2e74f19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tabular expected: {0: 0.7, 1: 0.2, 2: 0.1}\n",
            "Tabular sampled: {0: 0.703, 1: 0.194, 2: 0.103}\n",
            "Dense expected: {0: 0.699999988079071, 1: 0.20000000298023224, 2: 0.10000000149011612}\n",
            "Dense sampled: {0: 0.703, 1: 0.194, 2: 0.103}\n",
            "Distribution checks OK.\n"
          ]
        }
      ],
      "source": [
        "spec_dist = GameSpec(ranks=3, suits=1, hand_size=1, claim_kinds=('RankHigh',))\n",
        "rules_dist = Rules(spec_dist)\n",
        "iset = InfoSet(pid=0, hand=(1,), history=())\n",
        "legal = rules_dist.legal_actions_for(iset)\n",
        "\n",
        "tab = TabularPolicy(); tab.bind_rules(rules_dist)\n",
        "tab.set(iset, {legal[0]: 0.7, legal[1]: 0.2, legal[2]: 0.1})\n",
        "expected = tab.action_probs(iset)\n",
        "\n",
        "rng = random.Random(5)\n",
        "samples = [tab.sample(iset, rng) for _ in range(5000)]\n",
        "freq = freq_from_samples(samples)\n",
        "print('Tabular expected:', expected)\n",
        "print('Tabular sampled:', {k: round(v,3) for k,v in freq.items()})\n",
        "assert_close_dist(freq, expected, tol=0.07)\n",
        "\n",
        "dense = DenseTabularPolicy(spec_dist)\n",
        "dense.S[0, dense.hand_to_idx[iset.hand], :] = 0.0\n",
        "dense.S[0, dense.hand_to_idx[iset.hand], 1] = 0.7\n",
        "dense.S[0, dense.hand_to_idx[iset.hand], 2] = 0.2\n",
        "dense.S[0, dense.hand_to_idx[iset.hand], 3] = 0.1\n",
        "dense.recompute_likelihoods()\n",
        "expected_dense = dense.action_probs(iset)\n",
        "rng = random.Random(5)\n",
        "samples = [dense.sample(iset, rng) for _ in range(5000)]\n",
        "freq = freq_from_samples(samples)\n",
        "print('Dense expected:', expected_dense)\n",
        "print('Dense sampled:', {k: round(v,3) for k,v in freq.items()})\n",
        "assert_close_dist(freq, expected_dense, tol=0.07)\n",
        "print('Distribution checks OK.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13abfef5",
      "metadata": {},
      "source": [
        "## Correctness: Dense vs Tabular defaults\n",
        "Uniform defaults should match for the same infoset/history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9cedaa6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaults match for root and post-claim states.\n"
          ]
        }
      ],
      "source": [
        "spec_def = GameSpec(ranks=3, suits=2, hand_size=1, claim_kinds=('RankHigh','Pair'), suit_symmetry=True)\n",
        "rules_def = Rules(spec_def)\n",
        "tab = TabularPolicy(); tab.bind_rules(rules_def)\n",
        "dense = DenseTabularPolicy(spec_def)\n",
        "\n",
        "iset_root = InfoSet(pid=0, hand=(1,), history=())\n",
        "iset_after = InfoSet(pid=1, hand=(1,), history=(0,))\n",
        "\n",
        "tab_root = tab.action_probs(iset_root)\n",
        "dense_root = dense.action_probs(iset_root)\n",
        "tab_after = tab.action_probs(iset_after)\n",
        "dense_after = dense.action_probs(iset_after)\n",
        "\n",
        "assert_close_dist(tab_root, dense_root, tol=1e-6)\n",
        "assert_close_dist(tab_after, dense_after, tol=1e-6)\n",
        "print('Defaults match for root and post-claim states.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a655f5",
      "metadata": {},
      "source": [
        "## Correctness: Eval helpers invariants\n",
        "Sanity-check seat split results and play_match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fe97dbb5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled wins (rand vs BR): {'P1': 16, 'P2': 24}\n",
            "Seat-split win rates: {'A_seat1': 0.49, 'A_seat2': 0.0, 'B_seat1': 1.0, 'B_seat2': 0.51}\n",
            "Seat-split invariants OK.\n"
          ]
        }
      ],
      "source": [
        "spec_sm = GameSpec(ranks=2, suits=1, hand_size=1, claim_kinds=('RankHigh',))\n",
        "rules_sm = Rules(spec_sm)\n",
        "p_rand = RandomPolicy(); p_rand.bind_rules(rules_sm)\n",
        "p_br, _ = best_response_exact(spec_sm, p_rand)\n",
        "p_br.bind_rules(rules_sm)\n",
        "\n",
        "wins = play_match(Env(spec_sm), p_rand, p_br, episodes=40, seed=7)\n",
        "print('Sampled wins (rand vs BR):', wins)\n",
        "\n",
        "seats = eval_seats_split(spec_sm, p_rand, p_br, episodes=200, seed=9)\n",
        "print('Seat-split win rates:', seats)\n",
        "assert abs((seats['A_seat1'] + seats['B_seat2']) - 1.0) < 1e-9\n",
        "assert abs((seats['A_seat2'] + seats['B_seat1']) - 1.0) < 1e-9\n",
        "print('Seat-split invariants OK.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea8308b",
      "metadata": {},
      "source": [
        "## Performance: policy.sample microbench\n",
        "Measure per-sample runtime for common policy types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d45f155b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random sample: mean=0.446815s std=0.009302s median=0.449242s, per=0.000009 s\n",
            "Tabular sample: mean=0.715074s std=0.053046s median=0.711724s, per=0.000014 s\n",
            "Dense sample: mean=0.367926s std=0.001001s median=0.367808s, per=0.000007 s\n"
          ]
        }
      ],
      "source": [
        "spec_bench = GameSpec(ranks=3, suits=2, hand_size=1, claim_kinds=('RankHigh','Pair'), suit_symmetry=True)\n",
        "rules_bench = Rules(spec_bench)\n",
        "iset = InfoSet(pid=0, hand=(1,), history=())\n",
        "\n",
        "rand_pol = RandomPolicy(); rand_pol.bind_rules(rules_bench)\n",
        "tab_pol = TabularPolicy(); tab_pol.bind_rules(rules_bench)\n",
        "tab_pol.set(iset, {0: 0.6, 1: 0.2, 2: 0.2})\n",
        "dense_pol = DenseTabularPolicy(spec_bench)\n",
        "dense_pol.S[0, dense_pol.hand_to_idx[iset.hand], :] = 0.0\n",
        "dense_pol.S[0, dense_pol.hand_to_idx[iset.hand], 1] = 0.6\n",
        "dense_pol.S[0, dense_pol.hand_to_idx[iset.hand], 2] = 0.2\n",
        "dense_pol.S[0, dense_pol.hand_to_idx[iset.hand], 3] = 0.2\n",
        "dense_pol.recompute_likelihoods()\n",
        "\n",
        "N = 50000\n",
        "\n",
        "def bench_sample(policy):\n",
        "    r = random.Random(1)\n",
        "    for _ in range(N):\n",
        "        policy.sample(iset, r)\n",
        "\n",
        "for label, policy in [('Random', rand_pol), ('Tabular', tab_pol), ('Dense', dense_pol)]:\n",
        "    times = bench(lambda p=policy: bench_sample(p), runs=5, warmup=1)\n",
        "    report(f'{label} sample', times, units='s', per=N)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56024dd4",
      "metadata": {},
      "source": [
        "## Performance: play_match microbench\n",
        "Measure end-to-end match runtime for common policy pairings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57b85f06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rand vs Rand: mean=0.009850s std=0.000012s median=0.009856s, per=0.000049 s\n",
            "Rand vs Dense: mean=0.011031s std=0.000306s median=0.010897s, per=0.000055 s\n",
            "Dense vs Dense: mean=0.012268s std=0.000059s median=0.012263s, per=0.000061 s\n"
          ]
        }
      ],
      "source": [
        "spec_match = GameSpec(ranks=3, suits=2, hand_size=1, claim_kinds=('RankHigh','Pair'), suit_symmetry=True)\n",
        "rules_match = Rules(spec_match)\n",
        "rand_a = RandomPolicy(); rand_a.bind_rules(rules_match)\n",
        "rand_b = RandomPolicy(); rand_b.bind_rules(rules_match)\n",
        "dense_a = DenseTabularPolicy(spec_match)\n",
        "dense_b = DenseTabularPolicy(spec_match)\n",
        "\n",
        "EP = 200\n",
        "\n",
        "def bench_play(p1, p2):\n",
        "    env = Env(spec_match)\n",
        "    play_match(env, p1, p2, episodes=EP, seed=77)\n",
        "\n",
        "pairs = [\n",
        "    ('Rand vs Rand', rand_a, rand_b),\n",
        "    ('Rand vs Dense', rand_a, dense_a),\n",
        "    ('Dense vs Dense', dense_a, dense_b),\n",
        "]\n",
        "\n",
        "for label, p1, p2 in pairs:\n",
        "    times = bench(lambda a=p1, b=p2: bench_play(a, b), runs=5, warmup=1)\n",
        "    report(label, times, units='s', per=EP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d1d40fdf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from liars_poker.serialization import load_policy\n",
        "\n",
        "pol_a, spec_match = load_policy('C:/Users/adidh/Documents/liars_poker/artifacts/benchmark_runs/r5_s2_h1_h_ss___20260102-045537/policy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "567eee82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rand vs pol_a: mean=0.056633s std=0.001771s median=0.056024s, per=0.000057 s\n"
          ]
        }
      ],
      "source": [
        "rules_match = Rules(spec_match)\n",
        "pol_a.bind_rules(rules_match)\n",
        "rand_b = RandomPolicy(); rand_b.bind_rules(rules_match)\n",
        "\n",
        "\n",
        "EP = 1000\n",
        "\n",
        "def bench_play(p1, p2):\n",
        "    env = Env(spec_match)\n",
        "    play_match(env, p1, p2, episodes=EP, seed=77)\n",
        "\n",
        "pairs = [\n",
        "    ('Rand vs pol_a', pol_a, rand_b),\n",
        "]\n",
        "\n",
        "for label, p1, p2 in pairs:\n",
        "    times = bench(lambda a=p1, b=p2: bench_play(a, b), runs=5, warmup=1)\n",
        "    report(label, times, units='s', per=EP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0b4dfb41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'P1': 1074, 'P2': 926} P1=0.537 P2=0.463\n"
          ]
        }
      ],
      "source": [
        "# Fast-path sanity check (slow vs fast rollout win rates)\n",
        "from liars_poker.eval.match import play_match\n",
        "\n",
        "spec = GameSpec(ranks=3, suits=2, hand_size=1, claim_kinds=(\"RankHigh\",), suit_symmetry=True)\n",
        "env = Env(spec, seed=123)\n",
        "p1 = RandomPolicy(); p1.bind_rules(env.rules)\n",
        "p2 = RandomPolicy(); p2.bind_rules(env.rules)\n",
        "\n",
        "wins = play_match(env, p1, p2, episodes=2000, seed=999)\n",
        "rate_p1 = wins[\"P1\"] / max(1, sum(wins.values()))\n",
        "rate_p2 = wins[\"P2\"] / max(1, sum(wins.values()))\n",
        "print(wins, f\"P1={rate_p1:.3f} P2={rate_p2:.3f}\")\n",
        "assert abs(rate_p1 - rate_p2) < 0.08\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dff35c3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fast=0.169s slow=0.326s speedup=1.92x\n"
          ]
        }
      ],
      "source": [
        "# Fast vs slow rollout timing (rough comparison)\n",
        "import time\n",
        "import random\n",
        "\n",
        "def slow_play_match(env, p1, p2, episodes=1000, seed=0):\n",
        "    rng = random.Random(seed)\n",
        "    wins = {\"P1\": 0, \"P2\": 0}\n",
        "    p1.bind_rules(env.rules)\n",
        "    p2.bind_rules(env.rules)\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset(seed=rng.randint(0, 2_147_483_647))\n",
        "        p1.begin_episode(rng)\n",
        "        p2.begin_episode(rng)\n",
        "        while True:\n",
        "            if obs[\"terminal\"]:\n",
        "                winner = obs[\"winner\"]\n",
        "                if winner in wins:\n",
        "                    wins[winner] += 1\n",
        "                break\n",
        "            player = env.current_player()\n",
        "            policy = p1 if player == \"P1\" else p2\n",
        "            infoset = env.infoset_key(player)\n",
        "            action = policy.sample(infoset, rng)\n",
        "            obs = env.step(action)\n",
        "    return wins\n",
        "\n",
        "spec = GameSpec(ranks=7, suits=2, hand_size=1, claim_kinds=(\"RankHigh\",), suit_symmetry=True)\n",
        "episodes = 3000\n",
        "repeats = 3\n",
        "\n",
        "def bench(fn):\n",
        "    times = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn()\n",
        "        times.append(time.perf_counter() - t0)\n",
        "    return sum(times) / len(times)\n",
        "\n",
        "fast_env = Env(spec)\n",
        "slow_env = Env(spec)\n",
        "\n",
        "def run_fast():\n",
        "    p1 = RandomPolicy()\n",
        "    p2 = RandomPolicy()\n",
        "    play_match(fast_env, p1, p2, episodes=episodes, seed=123)\n",
        "\n",
        "def run_slow():\n",
        "    p1 = RandomPolicy()\n",
        "    p2 = RandomPolicy()\n",
        "    slow_play_match(slow_env, p1, p2, episodes=episodes, seed=123)\n",
        "\n",
        "fast_time = bench(run_fast)\n",
        "slow_time = bench(run_slow)\n",
        "speedup = slow_time / fast_time if fast_time > 0 else float('inf')\n",
        "print(f\"fast={fast_time:.3f}s slow={slow_time:.3f}s speedup={speedup:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "73647b5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fast=0.325s slow=0.515s speedup=1.59x\n"
          ]
        }
      ],
      "source": [
        "# More representative benchmark: DenseTabularPolicy vs DenseTabularPolicy\n",
        "import time\n",
        "import random\n",
        "\n",
        "def slow_play_match(env, p1, p2, episodes=3000, seed=0):\n",
        "    rng = random.Random(seed)\n",
        "    wins = {\"P1\": 0, \"P2\": 0}\n",
        "    p1.bind_rules(env.rules)\n",
        "    p2.bind_rules(env.rules)\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset(seed=rng.randint(0, 2_147_483_647))\n",
        "        p1.begin_episode(rng)\n",
        "        p2.begin_episode(rng)\n",
        "        while True:\n",
        "            if obs[\"terminal\"]:\n",
        "                wins[obs[\"winner\"]] += 1\n",
        "                break\n",
        "            player = env.current_player()\n",
        "            policy = p1 if player == \"P1\" else p2\n",
        "            infoset = env.infoset_key(player)\n",
        "            action = policy.sample(infoset, rng)\n",
        "            obs = env.step(action)\n",
        "    return wins\n",
        "\n",
        "spec = GameSpec(ranks=6, suits=3, hand_size=2, claim_kinds=(\"RankHigh\", \"Pair\"), suit_symmetry=True)\n",
        "fast_env = Env(spec)\n",
        "slow_env = Env(spec)\n",
        "\n",
        "p_fast = DenseTabularPolicy(spec)\n",
        "p_slow = DenseTabularPolicy(spec)\n",
        "\n",
        "episodes = 4000\n",
        "repeats = 3\n",
        "\n",
        "def bench(fn):\n",
        "    times = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn()\n",
        "        times.append(time.perf_counter() - t0)\n",
        "    return sum(times) / len(times)\n",
        "\n",
        "def run_fast():\n",
        "    play_match(fast_env, p_fast, p_fast, episodes=episodes, seed=123)\n",
        "\n",
        "def run_slow():\n",
        "    slow_play_match(slow_env, p_slow, p_slow, episodes=episodes, seed=123)\n",
        "\n",
        "fast_time = bench(run_fast)\n",
        "slow_time = bench(run_slow)\n",
        "print(f\"fast={fast_time:.3f}s slow={slow_time:.3f}s speedup={slow_time/fast_time:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "68e68828",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tabular edited (~1000 sets) :: fast=0.272s slow=0.426s speedup=1.57x\n"
          ]
        }
      ],
      "source": [
        "# Benchmark: Fast vs slow rollout timing using *TabularPolicy* with ~1000 manual probability edits\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from liars_poker.core import GameSpec, possible_starting_hands\n",
        "from liars_poker.env import Env, Rules\n",
        "from liars_poker.infoset import InfoSet, CALL, NO_CLAIM\n",
        "from liars_poker.policies.random import RandomPolicy\n",
        "from liars_poker.policies.tabular import TabularPolicy\n",
        "\n",
        "\n",
        "\n",
        "# ---- Slow reference match loop (unchanged) ----\n",
        "def slow_play_match(env, p1, p2, episodes=1000, seed=0):\n",
        "    rng = random.Random(seed)\n",
        "    wins = {\"P1\": 0, \"P2\": 0}\n",
        "    p1.bind_rules(env.rules)\n",
        "    p2.bind_rules(env.rules)\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset(seed=rng.randint(0, 2_147_483_647))\n",
        "        p1.begin_episode(rng)\n",
        "        p2.begin_episode(rng)\n",
        "        while True:\n",
        "            if obs[\"terminal\"]:\n",
        "                winner = obs[\"winner\"]\n",
        "                if winner in wins:\n",
        "                    wins[winner] += 1\n",
        "                break\n",
        "            player = env.current_player()\n",
        "            policy = p1 if player == \"P1\" else p2\n",
        "            infoset = env.infoset_key(player)\n",
        "            action = policy.sample(infoset, rng)\n",
        "            obs = env.step(action)\n",
        "    return wins\n",
        "\n",
        "\n",
        "# ---- Build a TabularPolicy and apply ~n_edits manual edits to .probs ----\n",
        "def random_history_nonterminal(rules: Rules, rng: random.Random, max_len: int) -> tuple[int, ...]:\n",
        "    \"\"\"\n",
        "    Generate a random *nonterminal* public history (no CALL), respecting strictly-increasing claim indices.\n",
        "    \"\"\"\n",
        "    history: list[int] = []\n",
        "    last_claim = NO_CLAIM\n",
        "    for _ in range(max_len):\n",
        "        last_idx = None if last_claim == NO_CLAIM else last_claim\n",
        "        legal = rules.legal_actions_from_last(last_idx)\n",
        "        # Exclude CALL so history stays nonterminal\n",
        "        legal_noncall = [a for a in legal if a != CALL]\n",
        "        if not legal_noncall:\n",
        "            break\n",
        "        a = rng.choice(legal_noncall)\n",
        "        history.append(a)\n",
        "        last_claim = a\n",
        "    return tuple(history)\n",
        "\n",
        "\n",
        "def make_edited_tabular_policy(\n",
        "    spec: GameSpec,\n",
        "    *,\n",
        "    n_edits: int = 1000,\n",
        "    seed: int = 0,\n",
        "    max_history_len: int = 12,\n",
        ") -> TabularPolicy:\n",
        "    \"\"\"\n",
        "    Start from a blank TabularPolicy (defaults to uniform when missing),\n",
        "    then write ~n_edits infoset distributions into .probs.\n",
        "    \"\"\"\n",
        "    rng = random.Random(seed)\n",
        "    rules = Rules(spec)\n",
        "\n",
        "    pol = TabularPolicy()\n",
        "    pol.bind_rules(rules)\n",
        "\n",
        "    hands = list(possible_starting_hands(spec))\n",
        "\n",
        "    # Precompute all possible opening claim actions (no CALL on first move)\n",
        "    k = len(rules.claims)\n",
        "\n",
        "    for _ in range(n_edits):\n",
        "        # Random nonterminal history; derive pid from parity of history length\n",
        "        hist = random_history_nonterminal(rules, rng, max_len=max_history_len)\n",
        "        pid = len(hist) % 2\n",
        "\n",
        "        # Random hand\n",
        "        hand = rng.choice(hands)\n",
        "        iset = InfoSet(pid=pid, hand=hand, history=hist)\n",
        "\n",
        "        legal = rules.legal_actions_for(iset)\n",
        "        if not legal:\n",
        "            continue\n",
        "\n",
        "        # Create a non-uniform distribution over legal actions\n",
        "        # (Dirichlet gives random probabilities, then we sparsify a bit)\n",
        "        w = np.asarray([rng.random() + 0.01 for _ in legal], dtype=float)\n",
        "\n",
        "        # Sparsify ~40% of actions to 0 to mimic realistic tabular sparsity\n",
        "        if len(legal) >= 3:\n",
        "            mask = np.asarray([rng.random() > 0.4 for _ in legal], dtype=bool)\n",
        "            if not mask.any():\n",
        "                mask[rng.randrange(len(legal))] = True\n",
        "            w = w * mask\n",
        "\n",
        "        if w.sum() <= 0:\n",
        "            # fallback: deterministic on a random legal\n",
        "            dist = {rng.choice(legal): 1.0}\n",
        "        else:\n",
        "            w = w / w.sum()\n",
        "            dist = {a: float(p) for a, p in zip(legal, w) if p > 0}\n",
        "\n",
        "        pol.set(iset, dist)\n",
        "\n",
        "    return pol\n",
        "\n",
        "\n",
        "# ---- Benchmark harness ----\n",
        "spec = GameSpec(ranks=7, suits=2, hand_size=1, claim_kinds=(\"RankHigh\",), suit_symmetry=True)\n",
        "episodes = 3000\n",
        "repeats = 3\n",
        "n_edits = 1000\n",
        "\n",
        "fast_env = Env(spec)\n",
        "slow_env = Env(spec)\n",
        "\n",
        "def bench(fn):\n",
        "    times = []\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        fn()\n",
        "        times.append(time.perf_counter() - t0)\n",
        "    return sum(times) / len(times)\n",
        "\n",
        "def run_fast():\n",
        "    # Build two tabular policies with manual edits\n",
        "    p1 = make_edited_tabular_policy(spec, n_edits=n_edits, seed=1)\n",
        "    p2 = make_edited_tabular_policy(spec, n_edits=n_edits, seed=2)\n",
        "    play_match(fast_env, p1, p2, episodes=episodes, seed=123)\n",
        "\n",
        "def run_slow():\n",
        "    p1 = make_edited_tabular_policy(spec, n_edits=n_edits, seed=1)\n",
        "    p2 = make_edited_tabular_policy(spec, n_edits=n_edits, seed=2)\n",
        "    slow_play_match(slow_env, p1, p2, episodes=episodes, seed=123)\n",
        "\n",
        "fast_time = bench(run_fast)\n",
        "slow_time = bench(run_slow)\n",
        "speedup = slow_time / fast_time if fast_time > 0 else float(\"inf\")\n",
        "print(f\"Tabular edited (~{n_edits} sets) :: fast={fast_time:.3f}s slow={slow_time:.3f}s speedup={speedup:.2f}x\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
